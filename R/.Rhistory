x = "Número do Jogo",
y = "Pontuação (PTS)") +
theme_minimal() +
xlim(0, 82) +
ylim(0,60)
print(p)
})
}
# Converta a coluna 'date' para um formato de data apropriado
df_wtp$date <- as.POSIXct(df_wtp$date)
df_player <- df_wtp %>%
filter(Name == player_name)
# Especifique a proporção de dados a serem usados para treinamento (por exemplo, 80%)
train_proportion <- 0.8
# Calcule o índice para dividir os dados
split_index <- floor(nrow(df_player) * train_proportion)
# Crie os conjuntos de treinamento e teste
train_data <- df_player[1:split_index, ]
test_data <- df_player[(split_index + 1):nrow(df_player), ]
# Converta os dados de treinamento para um objeto xts
train_time_series <- xts(train_data$PTS, order.by = train_data$date)
plot(train_time_series, main = "Irregular Time Series of PTS", ylab = "PTS", xlab = "Date")
# Ajuste um modelo ARIMA aos dados de treinamento
arima_model <- auto.arima(train_time_series)
# Preveja os valores para os dados de teste
predicted_values <- forecast(arima_model, h = length(test_data$PTS))
# Converta os dados de teste para um objeto xts
test_time_series <- xts(test_data$PTS, order.by = test_data$date)
# Compare os valores previstos com os valores reais nos dados de teste
accuracy <- accuracy(predicted_values, test_time_series)
print(accuracy)
# Converta os objetos xts em data frames para facilitar a plotagem
actual_train_data <- data.frame(Date = index(train_time_series), PTS = coredata(train_time_series))
predicted_train_data <- data.frame(Date = index(train_time_series), PTS = fitted(arima_model))
# Crie um data frame para plotar os valores reais e previstos
combined_train_data <- rbind(actual_train_data, predicted_train_data)
combined_train_data$Type <- c(rep("Actual", nrow(actual_train_data)), rep("Predicted", nrow(predicted_train_data)))
# Plot the actual and predicted values on the same graph for the training period
ggplot(combined_train_data, aes(x = Date, y = PTS, color = Type)) +
geom_line() +
labs(title = "Actual vs. Predicted PTS - Train Period",
x = "Date",
y = "PTS") +
theme_minimal()
# Converta os objetos xts em data frames para facilitar a plotagem
actual_test_data <- data.frame(Date = index(test_time_series), PTS = coredata(test_time_series))
predicted_test_data <- data.frame(Date = index(test_time_series), PTS = coredata(predicted_values$mean))
# Crie um data frame para plotar os valores reais e previstos
combined_test_data <- rbind(actual_test_data, predicted_test_data)
combined_test_data$Type <- c(rep("Actual", nrow(actual_test_data)), rep("Predicted", nrow(predicted_test_data)))
# Plot the actual and predicted values on the same graph for the test period
ggplot(combined_test_data, aes(x = Date, y = PTS, color = Type)) +
geom_line() +
labs(title = "Actual vs. Predicted PTS - Test Period",
x = "Date",
y = "PTS") +
theme_minimal()
# Converta a coluna 'date' para um formato de data apropriado
df_wtp$date <- as.POSIXct(df_wtp$date)
df_player <- df_wtp %>%
filter(Name == player_name)
# Especifique a proporção de dados a serem usados para treinamento (por exemplo, 80%)
train_proportion <- 0.8
# Calcule o índice para dividir os dados
split_index <- floor(nrow(df_player) * train_proportion)
# Crie os conjuntos de treinamento e teste
train_data <- df_player[1:split_index, ]
test_data <- df_player[(split_index + 1):nrow(df_player), ]
# Converta os dados de treinamento para um objeto xts
train_time_series <- xts(train_data$PTS, order.by = train_data$date)
plot(train_time_series, main = "Irregular Time Series of PTS", ylab = "PTS", xlab = "Date")
# Crie um data frame com as variáveis exógenas (todas as colunas iniciadas com "L5_" e "L10_", e "rest_days")
exog_vars <- as.matrix(train_data %>%
select(starts_with("L5_"), starts_with("L10_"), "rest_days", "season"))
# Ajuste um modelo ARIMA aos dados de treinamento com variáveis exógenas
x_arima_model <- auto.arima(train_time_series, seasonal = TRUE, xreg = exog_vars)
# Preveja os valores para os dados de teste
exog_vars_test <- as.matrix(test_data %>%
select(starts_with("L5_"), starts_with("L10_"), "rest_days", "season"))
predicted_values <- forecast(x_arima_model, xreg = exog_vars_test)
# Converta os dados de teste para um objeto xts
test_time_series <- xts(test_data$PTS, order.by = test_data$date)
# Compare os valores previstos com os valores reais nos dados de teste
accuracy <- accuracy(predicted_values, test_time_series)
print(accuracy)
# Converta os objetos xts em data frames para facilitar a plotagem
actual_train_data <- data.frame(Date = index(train_time_series), PTS = coredata(train_time_series))
predicted_train_data <- data.frame(Date = index(train_time_series), PTS = fitted(x_arima_model))
# Crie um data frame para plotar os valores reais e previstos
combined_train_data <- rbind(actual_train_data, predicted_train_data)
combined_train_data$Type <- c(rep("Actual", nrow(actual_train_data)), rep("Predicted", nrow(predicted_train_data)))
# Plot the actual and predicted values on the same graph for the training period
ggplot(combined_train_data, aes(x = Date, y = PTS, color = Type)) +
geom_line() +
labs(title = "Actual vs. Predicted PTS - Train Period",
x = "Date",
y = "PTS") +
theme_minimal()
# Converta os objetos xts em data frames para facilitar a plotagem
actual_test_data <- data.frame(Date = index(test_time_series), PTS = coredata(test_time_series))
predicted_test_data <- data.frame(Date = index(test_time_series), PTS = coredata(predicted_values$mean))
# Crie um data frame para plotar os valores reais e previstos
combined_test_data <- rbind(actual_test_data, predicted_test_data)
combined_test_data$Type <- c(rep("Actual", nrow(actual_test_data)), rep("Predicted", nrow(predicted_test_data)))
# Plot the actual and predicted values on the same graph for the test period
ggplot(combined_test_data, aes(x = Date, y = PTS, color = Type)) +
geom_line() +
labs(title = "Actual vs. Predicted PTS - Test Period",
x = "Date",
y = "PTS") +
theme_minimal()
df_player <- df_wtp %>%
filter(Name == player_name)
num_train_samples <- round(nrow(df_player) * .5)
num_val_samples <- round(nrow(df_player) * 0.25)
num_test_samples <- nrow(df_player) - num_train_samples - num_val_samples
train_df <- df_player[seq(num_train_samples), ]
val_df <- df_player[seq(from = nrow(train_df) + 1, length.out = num_val_samples),]
test_df <- df_player[seq(to = nrow(df_player), length.out = num_test_samples), ]
cat("num_train_samples:", nrow(train_df), "\n")
cat("num_val_samples:", nrow(val_df), "\n")
cat("num_test_samples:", nrow(test_df), "\n")
#Normalizing the data
input_data_colnames <- colnames(df_player %>%
select(starts_with("L5_"), starts_with("L10_"), "rest_days", "ppm"))
# Calculate mean and standard deviation for each column in input_data_colnames
mean_values <- lapply(train_df[input_data_colnames], mean)
sd_values <- lapply(train_df[input_data_colnames], sd)
# Combine the mean and standard deviation values into a dataframe
normalization_values <- data.frame(
Column = names(train_df[input_data_colnames]),
Mean = unlist(mean_values),
SD = unlist(sd_values)
)
# Print the normalization values
normalization_values
normalize_input_data <- function(df) {
normalize <- function(x, center, scale) {
(x - center) / scale
}
for (col_nm in input_data_colnames) {
col_nv <- normalization_values[normalization_values$Column == col_nm,]
df[[col_nm]] <- normalize(df[[col_nm]], col_nv$Mean, col_nv$SD)
}
return(df)
}
# Apply the normalization function to your dataframe (e.g., train_df)
normalized_train_df <- normalize_input_data(train_df)
sampling_rate <- 6
sequence_length <- 10
delay <- sampling_rate * (sequence_length  - 1)
batch_size <- 128
# Define the function to prepare inputs and targets
df_to_inputs_and_targets <- function(df, delay) {
inputs <- df[input_data_colnames] %>%
normalize_input_data() %>%
as.matrix()
targets <- as.array(df$PTS)
list(
head(inputs, -delay),
tail(targets, -delay)
)
}
# Define the function to create a time series dataset
make_dataset <- function(df) {
inputs_and_targets <- df_to_inputs_and_targets(df, delay)
inputs <- inputs_and_targets[[1]]
targets <- inputs_and_targets[[2]]
timeseries_dataset_from_array(
inputs, targets,
sampling_rate = sampling_rate,
sequence_length = sequence_length,
shuffle = TRUE,
batch_size = batch_size
)
}
train_dataset <- make_dataset(train_df)
val_dataset <- make_dataset(val_df)
test_dataset <- make_dataset(test_df)
library(tfdatasets)
options(tensorflow.extract.warn_negatives_pythonic = FALSE)
#Essas funções abaixo não estão vindo embutidas no tfdatasets por algum motivo e são necessárias para rodar o código abaixo.
as_py_function <- function(x) {
if (inherits(x, "python.builtin.function")) {
x
} else {
rlang::as_function(x)
}
}
as_integer_tensor <- function(x, dtype = tf$int64) {
# recurse over lists
if (is.list(x) || (is.numeric(x) && length(x) > 1))
lapply(x, function(elem) as_integer_tensor(elem, dtype))
else if (is.null(x))
x
else if (is_tensor(x))
tf$cast(x, dtype = dtype)
else
tf$constant(as.integer(x), dtype = dtype)
}
###
c(samples, targets) %<-% iter_next(as_iterator(train_dataset))
cat("samples shape: ", format(samples$shape), "\n",
"targets shape: ", format(targets$shape), "\n", sep = "")
#Training and evaluating a densely connected model
ncol_input_data <- length(input_data_colnames)
inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
outputs <- inputs %>%
layer_flatten() %>%
layer_dense(16, activation = "relu") %>%
layer_dense(1)
model <- keras_model(inputs, outputs)
early_stopping <- callback_early_stopping(
monitor = "val_loss",  # Monitor validation loss
patience = 10,         # Number of epochs with no improvement before stopping
verbose = 1            # Show messages about early stopping
)
model %>%
compile(
optimizer = "rmsprop",
loss = "mse",
metrics = list("mae", "mse")
)
history <- model %>%
fit(train_dataset,
epochs = 10,
validation_data = val_dataset,
callbacks = list(early_stopping))
model_weights_file <- "simple_model.keras"
save_model_weights_hdf5(model, model_weights_file)
eval_model <- evaluate(model, test_dataset)
mae <- eval_model[["mae"]]
mse <- eval_model[["mse"]]
rmse <- sqrt(mse)  # Calculate RMSE from MSE
sprintf("Test MAE: %.2f", mae)
sprintf("Test MSE: %.2f", mse)
sprintf("Test RMSE: %.2f", rmse)  # Print RMSE
#Plotting results
plot(history, metrics = "mae")
inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
outputs <- inputs %>%
layer_lstm(16) %>%
layer_dense(1)
model <- keras_model(inputs, outputs)
early_stopping <- callback_early_stopping(
monitor = "val_loss",  # Monitor validation loss
patience = 10,         # Number of epochs with no improvement before stopping
verbose = 1            # Show messages about early stopping
)
model %>%
compile(
optimizer = "rmsprop",
loss = "mse",
metrics = list("mae", "mse")
)
history <- model %>% fit(
train_dataset,
epochs = 50,
validation_data = val_dataset,
callbacks = list(early_stopping)
)
model_weights_file <- "lstm_model.keras"
save_model_weights_hdf5(model, model_weights_file)
eval_model <- evaluate(model, test_dataset)
mae <- eval_model[["mae"]]
mse <- eval_model[["mse"]]
rmse <- sqrt(mse)
sprintf("Test MAE: %.2f", mae)
sprintf("Test MSE: %.2f", mse)
sprintf("Test RMSE: %.2f", rmse)  # Print RMSE
plot(history)
#Listing 10.21 Training and evaluating a dropout-regularized LSTM
inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
outputs <- inputs %>%
layer_lstm(32, recurrent_dropout = 0.25) %>%
layer_dropout(0.5) %>%
layer_dense(1)
model <- keras_model(inputs, outputs)
early_stopping <- callback_early_stopping(
monitor = "val_loss",  # Monitor validation loss
patience = 10,         # Number of epochs with no improvement before stopping
verbose = 1            # Show messages about early stopping
)
model %>%
compile(
optimizer = "rmsprop",
loss = "mse",
metrics = list("mae", "mse")
)
history <- model %>% fit(
train_dataset,
epochs = 50,
validation_data = val_dataset,
callbacks = list(early_stopping)
)
model_weights_file <- "lstm_dropout_model.keras"
save_model_weights_hdf5(model, model_weights_file)
eval_model <- evaluate(model, test_dataset)
mae <- eval_model[["mae"]]
mse <- eval_model[["mse"]]
rmse <- sqrt(mse)
sprintf("Test MAE: %.2f", mae)
sprintf("Test RMSE: %.2f", rmse)
plot(history)
#Listing 10.22 Training and evaluating a dropout-regularized, stacked GRU model
inputs <- layer_input(shape = c(sequence_length, ncol_input_data))
outputs <- inputs %>%
layer_gru(32, recurrent_dropout = 0.5, return_sequences = TRUE) %>%
layer_gru(32, recurrent_dropout = 0.5) %>%
layer_dropout(0.5) %>%
layer_dense(1)
early_stopping <- callback_early_stopping(
monitor = "val_loss",  # Monitor validation loss
patience = 10,         # Number of epochs with no improvement before stopping
verbose = 1            # Show messages about early stopping
)
model %>%
compile(
optimizer = "rmsprop",
loss = "mse",
metrics = list("mae", "mse")
)
history <- model %>% fit(
train_dataset,
epochs = 50,
validation_data = val_dataset,
callbacks = list(early_stopping)
)
model_weights_file <- "lstm_dropout_gru_model.keras"
save_model_weights_hdf5(model, model_weights_file)
eval_model <- evaluate(model, test_dataset)
mae <- eval_model[["mae"]]
mse <- eval_model[["mse"]]
rmse <- sqrt(mse)
sprintf("Test MAE: %.2f", mae)
sprintf("Test RMSE: %.2f", rmse)
plot(history)
# Function to calculate the rolling median for a given column
calculate_rolling_median <- function(data, column, n) {
data <- data %>% group_by(Name)
column_name <- paste0("L", n, "_", column)
# Apply the function to calculate the rolling median using a window of size 10
data <- data %>%
mutate("{column_name}" := rollapply(.data[[column]], width = (n+1), FUN = calculate_median,
fill = NA, align = "right", partial = TRUE)) %>%
ungroup()
# Group the data by player name and opponent
data <- data %>% group_by(Name, opp)
n_opp <- floor(n/2)
column_name_opp <- paste0("L", n_opp, "_", column,"_opp")
# Apply the function to calculate the median using rolling window of size 10 per opponent
data <- data %>%
mutate("{column_name_opp}" := rollapply(.data[[column]], width = (n_opp+1), FUN = calculate_median, fill = NA, align = "right",partial=TRUE))
data <- data %>% ungroup()
return(data)
}
write.csv(df, file = "data/regular-season-cleaned.csv", row.names = FALSE)
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(purrr)
library(readr)
library(zoo)
df <- read_csv("data/regular_season.csv")
df <- subset(df, !is.na(MP) & MP != 0)
df <- na.fill(df, fill = 0)
df <- df %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date))
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(purrr)
library(readr)
library(zoo)
df <- na.fill(df, fill = 0)
df <- df %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date))
library(zoo)
```{r}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(purrr)
library(readr)
library(zoo)
library(lubridate)
df <- df %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date))
class(df)
df <- read_csv("data/regular_season.csv")
df <- subset(df, !is.na(MP) & MP != 0)
df <- as.data.frame(na.fill(df, fill = 0))
class(df)
df <- df %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date))
season_start_dates <- df %>%
distinct(season, year, month)
season_start_dates$season_month <- 1
current_season <- season_start_dates$season[1] #2016
current_season_month <- 1
for (i in 1:nrow(season_start_dates)) {
if (season_start_dates$season[i] != current_season) {
current_season_month <- 1
current_season <- season_start_dates$season[i]
current_season_month <- current_season_month + 1
}
else{
season_start_dates$season_month[i] <- current_season_month
current_season_month <- current_season_month + 1
}
}
df <- df %>%
left_join(season_start_dates, by = c("season", "year","month"))
df <- df %>% mutate(hour_float = as.numeric(hour) / 3600)
df
calculate_median <- function(x) {
if (length(x) == 1) {
return(0)
} else {
return(median(x[-length(x)], na.rm = TRUE))
}
}
# Function to calculate the rolling median for a given column
calculate_rolling_median <- function(data, column, n) {
data <- data %>% group_by(Name)
column_name <- paste0("L", n, "_", column)
# Apply the function to calculate the rolling median using a window of size 10
data <- data %>%
mutate("{column_name}" := rollapply(.data[[column]], width = (n+1), FUN = calculate_median,
fill = NA, align = "right", partial = TRUE)) %>%
ungroup()
# Group the data by player name and opponent
data <- data %>% group_by(Name, opp)
n_opp <- floor(n/2)
column_name_opp <- paste0("L", n_opp, "_", column,"_opp")
# Apply the function to calculate the median using rolling window of size 10 per opponent
data <- data %>%
mutate("{column_name_opp}" := rollapply(.data[[column]], width = (n_opp+1), FUN = calculate_median, fill = NA, align = "right",partial=TRUE))
data <- data %>% ungroup()
return(data)
}
columns_to_watch <- c("FG", "FGA", "3P", "3PA", "FT", "FTA", "TRB", "AST", "STL", "BLK", "TOV", "PF",
"+/-", "TS%", "eFG%", "USG%", "ORtg", "DRtg", "BPM", "PTS")
for (column in columns_to_watch) {
print(column)
df <- calculate_rolling_median(df, column, 10)
}
df <- read_csv("data/regular_season.csv")
df <- subset(df, !is.na(MP) & MP != 0 & Season >= 2016 & Season <= 2023)
df <- subset(df, !is.na(MP) & MP != 0 & season >= 2016 & season <= 2023)
df <- df %>%
mutate(year = lubridate::year(date),
month = lubridate::month(date),
day = lubridate::day(date))
season_start_dates <- df %>%
distinct(season, year, month)
season_start_dates$season_month <- 1
current_season <- season_start_dates$season[1]
current_season_month <- 1
for (i in 1:nrow(season_start_dates)) {
if (season_start_dates$season[i] != current_season) {
current_season_month <- 1
current_season <- season_start_dates$season[i]
current_season_month <- current_season_month + 1
}
else{
season_start_dates$season_month[i] <- current_season_month
current_season_month <- current_season_month + 1
}
}
df <- df %>%
left_join(season_start_dates, by = c("season", "year","month"))
calculate_median <- function(x) {
if (length(x) == 1) {
return(0)
} else {
return(median(x[-length(x)], na.rm = TRUE))
}
}
# Function to calculate the rolling median for a given column
calculate_rolling_median <- function(data, column, n) {
data <- data %>% group_by(Name)
column_name <- paste0("L", n, "_", column)
# Apply the function to calculate the rolling median using a window of size 10
data <- data %>%
mutate("{column_name}" := rollapply(.data[[column]], width = (n+1), FUN = calculate_median,
fill = NA, align = "right", partial = TRUE)) %>%
ungroup()
# Group the data by player name and opponent
data <- data %>% group_by(Name, opp)
n_opp <- floor(n/2)
column_name_opp <- paste0("L", n_opp, "_", column,"_opp")
# Apply the function to calculate the median using rolling window of size 10 per opponent
data <- data %>%
mutate("{column_name_opp}" := rollapply(.data[[column]], width = (n_opp+1), FUN = calculate_median, fill = NA, align = "right",partial=TRUE))
data <- data %>% ungroup()
return(data)
}
columns_to_watch <- c("FG", "FGA", "3P", "3PA", "FT", "FTA", "TRB", "AST", "STL", "BLK", "TOV", "PF",
"+/-", "TS%", "eFG%", "USG%", "ORtg", "DRtg", "BPM", "PTS")
for (column in columns_to_watch) {
print(column)
df <- calculate_rolling_median(df, column, 10)
}
df$ppm <- df$PTS / df$MP
df <- calculate_rolling_median(df, "ppm", 10)
write.csv(df, file = "data/regular-season-cleaned.csv", row.names = FALSE)
